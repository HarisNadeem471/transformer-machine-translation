{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "def load_sentences(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "train_en_path = '/mnt/data/train.en'\n",
    "train_ur_path = '/mnt/data/train.ur'\n",
    "train_en_sentences = load_sentences(train_en_path)\n",
    "train_ur_sentences = load_sentences(train_ur_path)\n",
    "\n",
    "# Ensure data alignment\n",
    "assert len(train_en_sentences) == len(train_ur_sentences), \"Mismatch in sentence counts\"\n",
    "\n",
    "# Step 3: Preprocess the data (clean and prepare)\n",
    "def preprocess_lines(lines):\n",
    "    return [line.replace('\\ufeff', '').strip() for line in lines]\n",
    "\n",
    "train_en_cleaned = preprocess_lines(train_en_sentences)\n",
    "train_ur_cleaned = preprocess_lines(train_ur_sentences)\n",
    "\n",
    "# Step 4: Train tokenizers and save them\n",
    "tokenizer_en = Tokenizer(models.BPE())\n",
    "trainer_en = trainers.BpeTrainer(special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"])\n",
    "tokenizer_en.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer_en.train_from_iterator(train_en_cleaned, trainer_en)\n",
    "tokenizer_en.save(\"/mnt/data/tokenizer_en.json\")\n",
    "\n",
    "tokenizer_ur = Tokenizer(models.BPE())\n",
    "trainer_ur = trainers.BpeTrainer(special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"])\n",
    "tokenizer_ur.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer_ur.train_from_iterator(train_ur_cleaned, trainer_ur)\n",
    "tokenizer_ur.save(\"/mnt/data/tokenizer_ur.json\")\n",
    "\n",
    "# Load tokenizers\n",
    "tokenizer_en = Tokenizer.from_file(\"/mnt/data/tokenizer_en.json\")\n",
    "tokenizer_ur = Tokenizer.from_file(\"/mnt/data/tokenizer_ur.json\")\n",
    "\n",
    "# Step 5: Tokenize the sentences for model input\n",
    "def tokenize_sentences(tokenizer, sentences, max_length=50):\n",
    "    encoded = [tokenizer.encode(sentence).ids for sentence in sentences]\n",
    "    return [seq[:max_length] + [0] * (max_length - len(seq)) for seq in encoded]\n",
    "\n",
    "X = tokenize_sentences(tokenizer_en, train_en_cleaned)\n",
    "y = tokenize_sentences(tokenizer_ur, train_ur_cleaned)\n",
    "\n",
    "# Step 6: Split the data for training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Create a PyTorch Dataset and DataLoader\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source, target):\n",
    "        self.source = torch.tensor(source, dtype=torch.long)\n",
    "        self.target = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.source[idx], self.target[idx]\n",
    "\n",
    "train_dataset = TranslationDataset(X_train, y_train)\n",
    "val_dataset = TranslationDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Step 8: Implement a basic Transformer model (Encoder-Decoder)\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, emb_dim=256, nhead=8, num_layers=3):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder = nn.Embedding(input_dim, emb_dim)\n",
    "        self.decoder = nn.Embedding(output_dim, emb_dim)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=emb_dim, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "        self.src_mask = None\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.encoder(src)\n",
    "        tgt_emb = self.decoder(tgt)\n",
    "        output = self.transformer(src_emb, tgt_emb)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "# Step 9: Initialize the model, loss function, and optimizer\n",
    "input_dim = len(tokenizer_en.get_vocab())\n",
    "output_dim = len(tokenizer_ur.get_vocab())\n",
    "\n",
    "model = TransformerModel(input_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Step 10: Training loop\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, tgt in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        tgt = tgt[:, 1:].reshape(-1)\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Training the model (use a smaller loop for initial testing)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    print(f'Epoch {epoch + 1}, Training Loss: {train_loss:.4f}')\n",
    "\n",
    "print(\"Model training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
